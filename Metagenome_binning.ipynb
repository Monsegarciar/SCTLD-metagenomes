{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b24a3a",
   "metadata": {},
   "source": [
    "### Preparing assembly files for binning\n",
    "To create a depth processing file, reads must be re-aligned to the contigs. This has been done using bowtie2 (can also be done using BWA). The next step would be to create a depth file with MetaBat2, convert that to be suitable for CONCOCT and MaxBin2, and then process these into bins. \n",
    "\n",
    "This is all assuming you have installed all of the softwares mentioned here. Use conda for quick install. If needed, the documentation for everything can be found here:\n",
    "\n",
    "MetaBat2: \n",
    "\n",
    "MaxBin2:\n",
    "\n",
    "CONCOCT: https://github.com/BinPro/CONCOCT\n",
    "\n",
    "CheckM: https://github.com/Ecogenomics/CheckM/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25da1fd1",
   "metadata": {},
   "source": [
    "#### MetaBat2\n",
    "The first piece of code here generates a fairly simple text file for the coverage of these files. *Still looking into whether the same file cannot be used for all binners.* So far no luck. The next set of code runs MetaBat2  (v2.10.2) using minContig 2500, minCV 1.0, minCVSum 1.0, maxP 95%, minS 60, and maxEdges 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a03a1a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-10-3eec9cf47e20>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [10]\u001b[0;36m\u001b[0m\n\u001b[0;31m    for f in Coral1 Coral2 Coral4 Coral5 T3-21-Mmea T3-4-Past\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#this creates a depth file for MetaBat\n",
    "#check names of the mapping files\n",
    "for f in Coral1 Coral2 Coral4 Coral5 T3-21-Mmea T3-4-Past\n",
    "do\n",
    "jgi_summarize_bam_contig_depths --outputDepth ../data/working/metabat_depth_\"$f\".txt ../../03_mapping/data/results/\"$f\".bam\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27d5f1db",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-87bf656f4fdd>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [11]\u001b[0;36m\u001b[0m\n\u001b[0;31m    for f in Coral1 Coral2 Coral4 Coral5 T3-21-Mmea T3-4-Past\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#this is the actual MetaBat2 script\n",
    "for f in Coral1 Coral2 Coral4 Coral5 T3-21-Mmea T3-4-Past\n",
    "do\n",
    "metabat2 -i ../../02_assembly/data/contigs-anvio/\"$f\".contigs-fixed.fa -a depth.txt \\\n",
    "-o ../data/working/metabat_depth_\"$f\".txt \\\n",
    "-v \\\n",
    "-m 1500 \\\n",
    "-s 200 \\#this is much lower than they do, kinda worried about this one... should I bin all the samples at once or bin them all seprarately? \\\n",
    "-l --unbinned \\\n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d33a97",
   "metadata": {},
   "source": [
    " #### CONCOCT\n",
    " This set of commands runs CONCOCT in its standard mode. It first creates a depth/coverage file for itself to use and then runs CONCOCT, with the standard settings. This means k-mer value is set to 4, minimum contig length is 1000, and CONCOCT runs on the exact amount of slots given to it by Hydra. \n",
    " \n",
    "CONCOCT creates a depth file out of the coverance created in the mapping step. It is key that this is all in the correct places before proceeding with binning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "926bf611",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-7db03b054bf7>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [6]\u001b[0;36m\u001b[0m\n\u001b[0;31m    for f in Coral1 Coral2 Coral4 Coral5 T3-21-Mmea T3-4-Past\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#this creates the CONCOCT depth file\n",
    "\n",
    "#this part cuts up the contigs into 10kb pieces for CONCOCT to use \n",
    "for f in Coral1 Coral2 Coral4 Coral5 T3-21-Mmea T3-4-Past\n",
    "do\n",
    "cut_up_fasta.py \"$f\".contigs-fixed.fa -c 500 -o 0 --merge_last -b ../data/working/\"$f\"_contigs_cut.bed > ../data/working/\"$f\"_contigs_cut.fa\n",
    "\n",
    "#this part estimates contig coverage #does my samtools install work?\n",
    "concoct_coverage_table.py ../data/working/\"$f\"_contigs_cut.bed ../../03_mapping/data/results/\"$f\".bam > ../data/working/coverage_table_\"$f\".tsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49664758",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-1ee438ed8a32>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [9]\u001b[0;36m\u001b[0m\n\u001b[0;31m    mkdir ../data/results/concoct_bins\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#CONCOCT script\n",
    "#make correct directories (can be omitted I think)\n",
    "mkdir ../data/results/concoct_bins\n",
    "mkdir ..data/working/concoct_temp\n",
    "#run the following for all your samples\n",
    "for f in Coral1 Coral2 Coral4 Coral5 T3-21-Mmea T3-4-Past\n",
    "do\n",
    "#this creates separate directories for all your samples, especially useful at later stages with many samples\n",
    "mkdir ../data/results/concoct_bins/\"$f\"_concoct_bins\n",
    "mkdir ../data/working/concoct_temp/\"$f\"_concoct_temp\n",
    "#this next bit actually runs CONCOCT itself\n",
    "concoct --composition_file ../data/working/\"$f\"_contigs_cut.fa --coverage_file ../data/working/coverage_table_\"$f\".tsv -t $NSLOTS -b ../data/working/concoct_temp/\"$f\"_concoct_temp/\n",
    "merge_cutup_clustering.py ../data/working/concoct_temp/\"$f\"_concoct_temp/clustering_gt1000.csv > ..data/working/concoct_temp/\"$f\"_concoct_temp/\"$f\"_clustering_merged.csv\n",
    "mkdir ../data/results/concoct_bins/\"$f\"_concoct_bins\n",
    "extract_fasta_bins.py \"$f\".contigs.fa ..data/working/concoct_temp/\"$f\"_concoct_temp/\"$f\"_clustering_merged.csv --output_path ../data/results/concoct_bins/\"$f\"_concoct_bins\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a53cc6",
   "metadata": {},
   "source": [
    "### GroopM\n",
    "This is another binning software that can be used. *Under construction*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10818cf",
   "metadata": {},
   "source": [
    "### Continuing\n",
    "You should now have 3 sets of bins, each created with a slightly different algorithm. It is now important to run the CheckM software with the script below and generate output files for all of them. This will inform you towards the quality of your bins and your contamination/completion rate. After this, you can proceed to the \"Refine Bins\" part of the workflow.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
